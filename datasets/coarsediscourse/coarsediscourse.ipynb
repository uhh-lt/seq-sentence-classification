{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, download\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = Path(\"./datasets/coarsediscourse\")\n",
    "test_path = datasets_path / \"coursediscourse_test.parquet\"\n",
    "train_path = datasets_path / \"coursediscourse_train.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(filename=download(\"reddit-coarse-discourse-corpus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_list: List[List[str]] = []\n",
    "labels_list: List[List[List[str]]] = []  \n",
    "\n",
    "for conversation in corpus.iter_conversations():\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    speaker_map = {speaker_id: f\"Speaker {(idx + 1)}\"  for idx, speaker_id in enumerate(conversation.get_speaker_ids())}\n",
    "    for utterance in conversation.iter_utterances():\n",
    "        text = utterance.text\n",
    "        text = \" \".join([text_segment for text_segment in text.split(\"\\n\") if len(text_segment.split()) > 1])\n",
    "        text = \" \".join(text.split(\"\\t\"))\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        sentences.append(text)\n",
    "        label = utterance.meta.get('majority_type', 'other')\n",
    "        if label is None:\n",
    "            label = 'other'\n",
    "        if label == \"negativereaction\":\n",
    "            label = \"negative reaction\"\n",
    "        labels.append(label)\n",
    "\n",
    "    assert len(sentences) == len(labels), \"Number of labels and sentences do not match\"\n",
    "    sentences_list.append(sentences)\n",
    "    labels_list.append(labels)\n",
    "\n",
    "assert len(sentences_list) == len(labels_list), \"Number of labels and sentences do not match\"\n",
    "\n",
    "# create dataframe\n",
    "df = pd.DataFrame({\"sentences\": sentences_list, \"labels\": labels_list})\n",
    "\n",
    "# unique labels\n",
    "unique_labels = set()\n",
    "for labels in labels_list:\n",
    "    unique_labels.update(labels)\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the df into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "train_df.to_parquet(train_path)\n",
    "test_df.to_parquet(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataframe\n",
    "train_df = pd.read_parquet(train_path)\n",
    "test_df = pd.read_parquet(test_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sent-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
