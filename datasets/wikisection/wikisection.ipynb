{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"de\"\n",
    "category = \"disease\" # city or disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy model\n",
    "nlp = spacy.load(\"de_core_news_sm\") if lang == \"de\" else spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable all other pipeline components and enable only the senter component\n",
    "nlp.disable_pipes(*[pipe for pipe in nlp.pipe_names if pipe != 'senter'])\n",
    "nlp.enable_pipe('senter')\n",
    "\n",
    "# Example usage\n",
    "doc = nlp(\"Tim ist toll. Er ist auch schlau. Er ist ein guter Mensch.\")\n",
    "sentences = [sent.text.strip() for sent in doc.sents]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = Path(f\"/home/tfischer/Development/seq-sentence-classification/datasets/wikisection/{lang}/{category}\")\n",
    "valid_path = datasets_path / f\"wikisection_{lang}_{category}_validation.json\"\n",
    "test_path = datasets_path / f\"wikisection_{lang}_{category}_test.json\"\n",
    "train_path = datasets_path / f\"wikisection_{lang}_{category}_train.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(json_path: Path):\n",
    "    data = json.loads(json_path.read_text())\n",
    "\n",
    "    sentences_list: List[List[str]] = []\n",
    "    labels_list: List[List[List[str]]] = []  \n",
    "\n",
    "    for wiki_article in data:\n",
    "\n",
    "        text = wiki_article[\"text\"]\n",
    "        annotations = wiki_article[\"annotations\"]\n",
    "\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for annotation in annotations:\n",
    "            section = text[annotation[\"begin\"]:annotation[\"begin\"] + annotation[\"length\"]].strip()\n",
    "\n",
    "            # sentence splitting with spacy\n",
    "            doc = nlp(section)\n",
    "            sents = []\n",
    "            for sent in doc.sents:\n",
    "                s = sent.text.strip()\n",
    "                s = s.replace(\"\\\\n\", \" \") # remove newlines\n",
    "                s = s.replace(\"\\\\t\", \" \") # remove tabs\n",
    "                s = \" \".join(s.split()) # remove multiple whitespaces\n",
    "                sents.append(s)\n",
    "\n",
    "            label = annotation[\"sectionLabel\"]\n",
    "            assert label.startswith(f\"{category}.\")\n",
    "            label = label[len(f\"{category}.\"):]\n",
    "\n",
    "            sentences.extend(sents)\n",
    "            labels.extend([label] * len(sents))\n",
    "\n",
    "        assert len(sentences) == len(labels), \"Number of labels and sentences do not match\"\n",
    "        sentences_list.append(sentences)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    assert len(labels_list) == len(sentences_list), \"Number of labels and sentences do not match\"\n",
    "\n",
    "    # create dataframe\n",
    "    df = pd.DataFrame({\"sentences\": sentences_list, \"labels\": labels_list})\n",
    "\n",
    "    # save dataframe\n",
    "    df.to_parquet(json_path.with_suffix(\".parquet\"))\n",
    "\n",
    "    # unique labels\n",
    "    unique_labels = set()\n",
    "    for labels in labels_list:\n",
    "        unique_labels.update(labels)\n",
    "    return unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = create_dataset(valid_path)\n",
    "l2 = create_dataset(test_path)\n",
    "l3 = create_dataset(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = l1.union(l2).union(l3)\n",
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read datasets\n",
    "valid_df = pd.read_parquet(valid_path.with_suffix(\".parquet\"))\n",
    "test_df = pd.read_parquet(test_path.with_suffix(\".parquet\"))\n",
    "train_df = pd.read_parquet(train_path.with_suffix(\".parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
